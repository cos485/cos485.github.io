---
layout: post
title: "Image classification using CIFAR10"
date:  Thu Mar 28 16:00:00 EST 2018
author: Davit Buniatyan
---

### No submission required
Google Colaboratory link for working online [CIFAR10](https://drive.google.com/file/d/1-fa0pYHhg-FKiWPWAGx03HO5k0Uq-0QQ/view?usp=sharing) (Open with Colaboratory > Open in Playground Mode)

In this tutorial, we will learn how to classify real images using same LeNet architecture used for MNIST. We will also apply state-of-the-art models like ResNet. We will use Pytorch with autograd feature, so there is no need to define the backward pass manually. We won't write the weight updates ourselves as everything will be covered by Pytorch. Pay careful attention to `train` function.

The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. [CIFAR](https://www.cs.toronto.edu/~kriz/cifar.html)   

1. Understanding the architecture.

   - How many kernels are in the convolutional layers?

   - How many fully connected layers are there, and how large are the weight matrices?

2. Architecture modifications. For example, you could vary the number of

   - feature maps in a convolutional layer

   - convolutional layers

   - maxpooling layers

   - fully connected layers

3. Training process

   - Try to add batch-normalization layer after each convolution. How does it affect the learning and final classificaiton error?

   - Data augmentation. Try to comment add data augmentation (comment out lines) and see how it affects the training.

   - Try to change the weight initialization.

   - Change optimizer to SGD. How the learning is affected? (after tuning the learning parameter)

4. Try state-of-the-art models by replacing the LeNet nn.module object  [here](https://github.com/kuangliu/pytorch-cifar/tree/master/models) and see how training differs. (Turn on GPU)

   - [googlenet](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)

   - [vggnet](https://arxiv.org/pdf/1409.1556.pdf)

   - [resnet](https://arxiv.org/pdf/1512.03385.pdf)

   - [densnet](https://arxiv.org/pdf/1608.06993.pdf)
