---
layout: post_2019
title:  "Problem Set 0"
date:   2019-02-04
author: Zoe Ashwood
---
<style>
.center-image
{
    margin: 0 auto;
    display: block;
}
</style>

$$
\DeclareMathOperator*\trace{Tr}
\DeclareMathOperator*\argmax{argmax}
\DeclareMathOperator*\argmin{argmin}
\DeclareMathOperator*\prob{\mathbb{P}}
$$

#### This homework is provided as a linear algebra and multivariate calculus refresher, and by doing this homework, you may also derive some results that will prove useful later in the class. Completion is optional, but recommended.  Stop by Office Hours, or reach out on Piazza if you have questions.

### Gradient

In this class, we will learn about Gradient Descent, which is a numerical method to find local minima of functions when no closed form exists. The method involves initializing the parameters of the function and then taking small steps in the direction of the negative gradient until the gradient goes to zero (in which case you have found a local minimum). In particular, if you have a function $$f(\vec{\theta})$$, you will choose some initial $$\vec{\theta_{0}}$$, and then continue to update $$\vec{\theta}$$ according to $$\vec{\theta}_{t} = \theta_{t-1} - \eta \nabla{f(\vec{\theta}_{t})}$$, where $$\eta$$ is some fixed constant (e.g. 0.01 or 0.001).  In this question, we will explore the consequences of following the negative gradient, and investigate when gradient descent succeeds in allowing us to find local minima.

![gradient1](https://cos485.github.io/assets/gradients1.png){:height="500px"}
![gradient2](https://cos485.github.io/assets/gradients2.png){:height="500px"}

1. In the first Figure above, we show contours for the function $$f(x, y) = 2e^{-x^{2}-y^{2}} - 2e^{-(x-1)^{2}-(y-1)^{2}}$$  At the six locations marked by crosses, draw arrows to indicate the direction of the negative gradient vector, namely $$-\nabla f(x,y)$$.

2. In the second figure, we show contours for negative 2D Gaussian distribution, $$f(x,y) = - \mathcal{N}(\mathbf{\mu}, \mathbf{\Sigma})$$, with $$\mathbf{\mu} = [0,0]$$ and $$\mathbf{\Sigma} = \begin{pmatrix} 1 & -0.8 \\ -0.8 & 1 \end{pmatrix}$$. At the six locations marked by crosses, draw arrows to indicate the direction of the negative gradient vector, namely $$-\nabla f(x,y)$$

3. Based on your answers to the above, discuss when gradient descent is likely to succeed in enabling us to find local minima, and when it is likely to fail.




### Hyperplanes
1.  What is the definition of a hyperplane?
2.  In 2D, what does a hyperplane correspond to?
3.  In 3D, what does a hyperplane correspond to?


### Linear Algebra and Index Notation
Let $$A \in \mathbb{R}^{m \times n}$$, $$B \in \mathbb{R}^{n \times p}$$ and $$x,y \in \mathbb{R}^{n}$$.  
- We can write the scalar product of $$x$$ and $$y$$ in index notation as $$x^{T}y = \sum_{i}x_{i}y_{i}$$.  
- We can write the product of two matrices, $$A$$ and $$B$$ as $$[AB]_{ij} = \sum_{k}A_{ik}B_{kj}$$  $$\forall i \in \{1,...,m\} \text{ and } \forall j \in \{1,...,p\}$$.  

Being able to write matrix-matrix, matrix-vector, or vector-vector products in index notation will prove useful when we are deriving backpropagation (i.e. applying the chain rule to functions of vectors); and index notation also allows us to introduce objects that have more than two indices (i.e. objects that are tensors instead of matrices).

1. Let $$z = Ax$$ where $$A \in \mathbb{R}^{m \times n}$$ and $$x \in \mathbb{R}^{n}$$.  Use index notation to represent the elements of $$z$$ as a sum.
2. Let $$z = x^{T}A$$ where $$A \in \mathbb{R}^{m \times n}$$ and $$x \in \mathbb{R}^{m}$$.  Use index notation to represent the elements of $$z$$ as a sum.
3. Let $$A, B, C \in \mathbb{R}^{n \times n}$$. Write $$[(AB)C]_{ij}$$ using index notation.
4. Let $$A \in \mathbb{R}^{n \times n}$$.  The trace of a matrix, $$tr(A)$$ is defined as the sum of its diagonal elements. Write the trace of A as a sum using index notation, and use your result to show that $$tr(AB) = tr(BA)$$
5. Let $$A \in \mathbb{R}^{m \times n}$$ and $$x \in \mathbb{R}^{m}$$. Let $$z = x^{T}Ax$$. What are the dimensions of z?  Write $$z$$ as a sum using index notation.


### Matrix Calculus
Let $$f: \mathbb{R}^{m \times n} \rightarrow \mathbb{R}$$.  That is, $$f$$ takes in matrix $$A$$ and returns a scalar, $$f(A)$$. The gradient of $$f$$ with respect to $$A \in \mathbb{R}^{m \times n}$$ is the matrix of partial derivatives:

\\[
\begin{equation}
    \nabla_{A} f \in \mathbb{R}^{m \times n} = \begin{pmatrix}
    \frac{\partial f}{\partial A_{11}} & \frac{\partial f}{\partial A_{12}} & \frac{\partial f}{\partial A_{13}} & \dots  & \frac{\partial f}{\partial A_{1n}} \cr
    \frac{\partial f}{\partial A_{21}} & \frac{\partial f}{\partial A_{22}} & \frac{\partial f}{\partial A_{23}} & \dots  & \frac{\partial f}{\partial A_{2n}} \cr
    \vdots & \vdots & \vdots & \ddots & \vdots \cr
    \frac{\partial f}{\partial A_{m1}} & \frac{\partial f}{\partial A_{m2}} & \frac{\partial f}{\partial A_{m3}} & \dots  & \frac{\partial f}{\partial A_{mn}} \cr
\end{pmatrix}
\end{equation}
\\]


In particular, 
<p> $$[\nabla_{A} f(A)]_{ij} =  \dfrac{\partial f(A)}{\partial A_{ij}}$$ </p>

Furthermore, the gradient of a function of a vector is just a special case of the above.  In particular, if $$g: \mathbb{R}^{n} \rightarrow \mathbb{R}$$, then $$[\nabla_{x} g(x)]_{i} = \dfrac{\partial g(x)}{\partial x_{i}}$$.

1. For $$x \in \mathbb{R}^{n}$$, let $$f(x) = b^{T}x$$ for some known vector $$b \in \mathbb{R}^{n}$$.  What is $$\dfrac{\partial f}{\partial x_{j}}$$?  Write the gradient as a vector too - namely, write $$\nabla_{x}f(x)$$.
2. For $$x \in \mathbb{R}^{n}$$, let $$f(x) = x^{T}Ax$$ for some known matrix $$A \in \mathbb{R}^{n \times n}$$.  What is $$\dfrac{\partial f}{\partial x_{j}}$$?  Write the gradient as a vector too - namely, write $$\nabla_{x}f(x)$$.
3. When performing least squares regression, we are given a data matrix $$A \in \mathbb{R}^{m \times n}$$ and a set of target values $$b \in \mathbb{R}^{m}$$, and the goal is to find a vector $$x \in \mathbb{R}^{n}$$ that minimizes the square of Euclidean distance between $$Ax$$ and $$b$$, namely $$\left \lVert Ax -b \right \rVert^{2}$$.
	- Writing $$\left \lVert Ax -b \right \rVert^{2}$$ as $$(Ax -b)^{T}(Ax-b) = x^{T}A^{T}Ax -2b^{T}Ax + b^{T}b$$, calculate the gradient of the squared Euclidean distance (which we will hereafter refer to as $$D$$) with respect to $$x$$; that is, calculate $$\nabla_{x}D$$. 
	- Set your resulting expression to zero and rearrange to get an expression for $$x$$.  You have just derived the normal-equations: the closed-form solution to least-squares regression.


### Chain Rule and Sum Rule of Differentiation

You will encounter batch normalization as a concept later in the course. Right now, we'll treat batch normalization as a function: $$B(\vec{x}): \mathbb{R}^{m} \rightarrow \mathbb{R}^{m}$$ that is defined as follows:

\\[
\begin{equation}
    B(\vec{x})_{i} = \gamma \hat{x}_i + \beta    
\end{equation}
\\]

where $$\gamma$$ and $$\beta$$ are constants, and $$\hat{x}_{i}$$ is defined as:

\\[
\begin{equation}
    \hat{x}_i =  \frac{\vec{x}_i - \mu_B}{ \sqrt{ \sigma_B^{2} + \epsilon}}    
\end{equation}
\\]

for $$i \in \{1,...,m\}$$, $$\epsilon$$ is a constant (to prevent division by zero) and 
\\[
\begin{equation}
    \sigma_B^{2} = \dfrac{1}{m} \sum_{i = 1}^{m} (\vec{x}_i - \mu_B)^{2}    
\end{equation}
\\]

and 
\\[
\begin{equation}
    \mu_{B} = \dfrac{1}{m} \sum_{i = 1}^{m} \vec{x}_{i} 
\end{equation}
\\]

Let's imagine that there is a loss function $$L(B(\vec{x})): \mathbb{R}^{m} \rightarrow \mathbb{R}$$, and you know the derivative $$\nabla L(B(\vec{x}))$$ (where $$[\nabla L(B(\vec{x}))]_{i} \equiv \dfrac{\partial L}{\partial [B(\vec{x})]_{i}}$$).  In this question, we will derive the derivative of the loss function, $$L$$, with respect to the input $$\vec{x}$$, namely $$\nabla L(\vec{x})$$. 


1. Draw the dependency graph between variables $$\vec{x}$$, $$\vec{\hat{x}}$$, $$\mu_{B}$$, $$\sigma_{B}^{2}$$ and $$B(\vec{x})$$.
2. Calculate $$\dfrac{\partial L}{\partial \vec{\hat{x}}}$$ in terms of $$\dfrac{\partial L}{\partial B(\vec{x})}$$ and other parameters defined above.
3. Calculate  $$\dfrac{\partial L}{\partial \sigma_{B}^{2}}$$ in terms of $$\dfrac{\partial L}{\partial \vec{\hat{x}}}$$, $$\dfrac{\partial L}{\partial B(\vec{x})}$$ and other parameters defined above.
4. Calculate  $$\dfrac{\partial L}{\partial \mu_{B}}$$ in terms of $$\dfrac{\partial L}{\partial \sigma_{B}^{2}}$$, $$\dfrac{\partial L}{\partial \vec{\hat{x}}}$$, $$\dfrac{\partial L}{\partial B(\vec{x})}$$ and other parameters defined above.
5. Calculate  $$\nabla L(\vec{x})$$ in terms of $$\dfrac{\partial L}{\partial \mu_{B}}$$, $$\dfrac{\partial L}{\partial \sigma_{B}^{2}}$$, $$\dfrac{\partial L}{\partial \vec{\hat{x}}}$$, $$\dfrac{\partial L}{\partial B(\vec{x})}$$ and other parameters defined above.



### Derivative of 1D Convolution


The 1D convolution of 2 vectors, $$\vec{w} \in \mathbb{R}^{m}$$ and $$\vec{s} \in \mathbb{R}^{n}$$ is defined as
<p>$$ [\vec{w} * \vec{s}]_{i} \equiv \sum_{j = 0}^{n-1}w_{i-j}s_{j} $$</p>
where $$w_{i} = 0$$ if $$i < 0$$ or $$i \geq m$$.

1. Calculate the convolution of $$\vec{w} = (2,3,4)$$ with $$\vec{s} = (1, 3, 5, 7)$$ for $$i \in \{0,...,5\}$$. 
2. Imagine there is a loss function, $$L(\vec{w} * \vec{s}): \mathbb{R}^{n+m-1} \rightarrow \mathbb{R}$$, and we know $$\dfrac{\partial L}{\partial [\vec{w} * \vec{s}]_{i}} \forall i \in \{0,...,n+m-2\}$$.  Calculate the derivatives $$\dfrac{\partial L}{\partial s_{j}}$$ for $$j \in \{0, ..., n-1\}$$ in terms of the $$\dfrac{\partial L}{\partial [\vec{w} * \vec{s}]_{i}}$$ values; write these as convolutions.  
3. Calculate the derivatives $$\dfrac{\partial L}{\partial w_{k}}$$ for $$k \in \{0, ..., m-1\}$$ in terms of the $$\dfrac{\partial L}{\partial [\vec{w} * \vec{s}]_{i}}$$ values; write these as convolutions.  

### NumPy Practice
"NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more." (From https://docs.scipy.org/doc/numpy-1.10.1/user/whatisnumpy.html)

1. Write a function that takes in two 2D numpy arrays, $$A$$ and $$B$$, of the same size and adds the arrays elementwise - that is $$(A+B)_{ij} = A_{ij} + B_{ij}$$.  Do not allow yourself to use any "for" loops, instead use NumPy functions.  Compare the length of time to run this function to the time taken if you do use one or more for loops to complete this operation.  
2. Write a function that takes in two 3D numpy arrays of the same size, $$A$$ and $$B$$, and that computes the scalar product along the third dimension.  That is, return an output array $$C$$, where $$C_{ij} = \sum_{k} A_{ijk} B_{ijk}$$.  Again, do not allow yourself to use any "for" loops when performing this operation.






