---
layout: post_2019
title: "Problem Set 4"
date:  Mar 5, 2019
---
\\(
\DeclareMathOperator*\argmax{argmax}
\\)

### Due at 3:00 pm on Tuesday, Mar. 12. Please submit to Gradescope and follow the general [guidelines](https://cos485.github.io/2019/02/12/homework-guidelines.htm) regarding homework assignments.

1. Open the [ConvolutionBasics](https://drive.google.com/file/d/1ux8YSbhwE0EPETm-qSjJumhdl2VZLwz4/view?usp=sharing) notebook.  Your task is to fill in the missing parts of the code.  Submit the completed notebook.
   - Exercise 1 helps you think about the definition of 1D convolution.

   - Exercise 2 shows how to generalize exercise 1 to a 2D (and higher).

   - Exercise 3 illustrates that 1D convolution is equivalent to multiplication by a Toeplitz matrix, which is a way of seeing that a convolutional net is a special case of a neural net. 

   - Exercise 4 adds batches and feature maps completing a full convolutional layer.
   
2. Design a ConvNet that detects this pattern anywhere in a binary input image.
\\[
\begin{array}{cccc}
1 &1 &1 &1\cr
1 &0 &0 &1\cr
1 &0 &0 &1\cr
1 &1 &1 &1
\end{array}
\\]
(When detecting this pattern, it doesn't matter what the input is outside the $$4\times 4$$ block.)
The output of the ConvNet should be a single feature map that encodes the existence of the above pattern at the locations in the input image.  Here's a constraint on your solution: the maximum allowable size of your kernels is $$3\times 3$$.  Use the Heaviside step function as your nonlinearity (since we are modeling Boolean functions and don't care about backprop here). Write code to implement your ConvNet on $$9\times 9$$ input images. Put this code in a separate notebook and title it `ManualConvNet`.

   - In your submitted notebook, one cell should contain the kernels in each layer, along with biases/thresholds. (Hint: no pooling is necessary.)  Explain very briefly why this is a correct answer.
 
   - Show tests of your code on a few input images.

3. The [LeNet](https://drive.google.com/file/d/1NFX17hy6gEpuuIeEd3OeWCI3kpqxHWOn/view?usp=sharing) notebook contains sample code to train LeNet on MNIST. Modify the code to include biases in the activity calculations and add all weight updates. As an indication for how well your code is doing: You should have reached an average classication error lower than 0.1 at about 5,000 iterations (using the default parameters).
