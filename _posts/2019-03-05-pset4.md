---
layout: post_2019
title: "Problem Set 4"
date:  Tue Mar 5  23:59:00 EST 2019
---
\\(
\DeclareMathOperator*\argmax{argmax}
\\)

### Due at 3:00 pm on Tuesday, Mar. 12. Please submit to Gradescope and follow the general [guidelines](https://cos485.github.io/2019/02/12/homework-guidelines.htm) regarding homework assignments.

1. Open the [ConvolutionBasics](https://drive.google.com/file/d/1ux8YSbhwE0EPETm-qSjJumhdl2VZLwz4/view?usp=sharing) notebook.  Your task is to fill in the missing parts of the code.  Submit the completed notebook.
   - Exercise 1 helps you think about the definition of 1D convolution.

   - Exercise 2 shows how to generalize exercise 1 to a 2D (and higher).

   - Exercise 3 illustrates that 1D convolution is equivalent to multiplication by a Toeplitz matrix, which is a way of seeing that a convolutional net is a special case of a neural net. 

   - Exercise 4 adds batches and feature maps completing a full convolutional layer.
   
2. Design a ConvNet that detects this pattern anywhere in a binary input image.
\\[
\begin{array}{cccc}
1 &1 &1 &1\cr
1 &0 &0 &1\cr
1 &0 &0 &1\cr
1 &1 &1 &1
\end{array}
\\]
(When detecting this pattern, it doesn't matter what the input is outside the $$4\times 4$$ block.)
The output of the ConvNet should be a single feature map that encodes the existence of the above pattern at the locations in the input image.  Here's a constraint on your solution: the maximum allowable size of your kernels is $$3\times 3$$.  Use the Heaviside step function as your nonlinearity (since we are modeling Boolean functions and don't care about backprop here). Write code to implement your ConvNet on $$9\times 9$$ input images. Put this code in a separate notebook and title it `ManualConvNet`.

   - In your submitted notebook, one cell should contain the kernels in each layer, along with biases/thresholds. (Hint: no pooling is necessary.)  Explain very briefly why this is a correct answer.
 
   - Show tests of your code on a few input images.

3. The [LeNet](https://drive.google.com/open?id=1zsezKIGPy6A_b2PMyGuWNg0xvcY3PObP) notebook contains sample code to train LeNet on MNIST.  Modify the code to include biases in all the layers, and add code for training the biases.  Note that all neurons in a feature map share the same bias.  Run the code to show that the learning curve goes down.  (This code is written for pedagogical purposes, not efficiency.  It's slow so we are not bothering with any time-consuming model selection.  We will do that later with a deep learning framework.)
