---
layout: post_2019
title: "Problem Set 4"
date:  Mar 5, 15:00:00 EST 2020
---
\\(
\DeclareMathOperator*\argmax{argmax}
\\)

### Due at 3:00 pm on Thursday, Mar. 12. Please submit to [Gradescope](https://www.gradescope.com/courses/87954/assignments/377110) and follow the general [guidelines](https://cos485.github.io/2020/02/03/homework-guidelines.html) regarding homework assignments.

1. Open the [ConvolutionBasics](https://colab.research.google.com/drive/1Uq7ofdLl9CkceFJi5ZIrr1wjz8g9ljbD) notebook. Your task is to observe the results of the code, fill in the missing parts, and answer questions. Submit the completed notebook.
   - Exercise 1 helps you learn how 2D convolution operators act on images.

   - Exercise 2 shows you different types of padding.

   - Exercise 3 adds batches and channels completing a full convolutional layer.
   
2. Design a ConvNet that detects this pattern anywhere in a binary input image.
\\[
\begin{array}{cccc}
1 &1 &1 &1\cr
1 &0 &0 &1\cr
1 &0 &0 &1\cr
1 &1 &1 &1
\end{array}
\\]
(When detecting this pattern, it doesn't matter what the input is outside the $$4\times 4$$ block.)
The output of the ConvNet should be a single feature map that encodes the existence of the above pattern at the locations in the input image. Here's a constraint on your solution: the maximum allowable size of your kernels is $$3\times 3$$.  Use the Heaviside step function as your nonlinearity (since we are modeling Boolean functions and don't care about backprop here). Write code to implement your ConvNet on $$9\times 9$$ input images. Put this code in a separate notebook and title it `ManualConvNet.ipynb`.

   - In your submitted notebook, one cell should contain the kernels in each layer, along with biases/thresholds. (Hint: no pooling is necessary.)  Explain very briefly why this is a correct answer.
 
   - Show tests of your code on a few input images.

3.The [LeNet](https://colab.research.google.com/drive/1Neif4TvzrHj9NUEaoZS2Y24HX5tVZFxo) notebook contains sample code to train a LeNet on MNIST. Please read it carefully and complete the starter code to define layers and forward pass. Run the code to show that the learning curve goes down. Also implement a variant of LeNet with ReLU activation by mimicking the starter code. (This code is written for pedagogical purposes, not efficiency. Itâ€™s slow so we are not bothering with any time-consuming model selection.)
